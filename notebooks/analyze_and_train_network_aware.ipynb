{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1fb5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Enhanced Training with Network-Aware Graph Structure\n",
    "This version incorporates network features into the graph edges\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abeff47",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"NETWORK-AWARE HYBRID GNN - ENHANCED GRAPH STRUCTURE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265873fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1f418f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.nn import GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4200dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For visualization\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    PLOT_AVAILABLE = True\n",
    "    plt.style.use('seaborn-v0_8-darkgrid')\n",
    "    sns.set_palette(\"husl\")\n",
    "except ImportError:\n",
    "    print(\"âš ï¸  matplotlib/seaborn not available. Install with: pip install matplotlib seaborn scikit-learn\")\n",
    "    PLOT_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168df7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "HIDDEN_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "DROPOUT = 0.4\n",
    "LEARNING_RATE = 0.0005\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 100\n",
    "EARLY_STOP_PATIENCE = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4d45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"\\nDevice: {device}\")\n",
    "print(f\"Hidden Dim: {HIDDEN_DIM}, Heads: {NUM_HEADS}, Batch: {BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0767098a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# DATA ANALYSIS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA ANALYSIS\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc96748",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('data/sequences_with_network_features.json')\n",
    "with open(data_path) as f:\n",
    "    training_pairs = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abaecf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ… Loaded {len(training_pairs)} training pairs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8d90b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variant distribution\n",
    "variant_counts = Counter(p['variant_type'] for p in training_pairs)\n",
    "print(f\"\\nVariant Distribution:\")\n",
    "for variant, count in variant_counts.items():\n",
    "    print(f\"  {variant:12s}: {count:5d} ({100*count/len(training_pairs):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e646d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top transitions\n",
    "transitions = [(p['current_attack'], p['next_attack']) for p in training_pairs]\n",
    "transition_counts = Counter(transitions)\n",
    "print(f\"\\nTop 10 Transitions:\")\n",
    "for (curr, nxt), count in transition_counts.most_common(10):\n",
    "    print(f\"  {curr} â†’ {nxt}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a64812a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network features analysis\n",
    "all_features = np.array([p['network_features'] for p in training_pairs])\n",
    "print(f\"\\nNetwork Features (3s window):\")\n",
    "print(f\"  Packet count - Mean: {all_features[:, 0].mean():.2f}, Std: {all_features[:, 0].std():.2f}\")\n",
    "print(f\"  Byte count   - Mean: {all_features[:, 1].mean():.2f}, Std: {all_features[:, 1].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61f9fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# BUILD NETWORK-AWARE GRAPH\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"BUILDING NETWORK-AWARE GRAPH\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93449f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_techniques = sorted(set(p['current_attack'] for p in training_pairs) | \n",
    "                       set(p['next_attack'] for p in training_pairs))\n",
    "technique_to_idx = {tech: idx for idx, tech in enumerate(all_techniques)}\n",
    "idx_to_technique = {idx: tech for tech, idx in technique_to_idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3db899",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_techniques = len(all_techniques)\n",
    "num_network_features = len(training_pairs[0]['network_features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4134091e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ… {num_techniques} techniques, {num_network_features} network features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016a264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build graph with network feature statistics per edge\n",
    "print(\"\\nðŸ“Š Aggregating network features per transition...\")\n",
    "edge_features_dict = defaultdict(lambda: defaultdict(list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2a0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in training_pairs:\n",
    "    curr = pair['current_attack']\n",
    "    nxt = pair['next_attack']\n",
    "    net_feat = pair['network_features']\n",
    "    \n",
    "    edge_features_dict[curr][nxt].append(net_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7277bc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multi-dimensional edge features\n",
    "print(\"ðŸ“Š Creating multi-dimensional edge features...\")\n",
    "edge_list, edge_features = [], []\n",
    "total_transitions = len(training_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for curr_tech, next_techs in edge_features_dict.items():\n",
    "    curr_idx = technique_to_idx[curr_tech]\n",
    "    \n",
    "    for next_tech, net_feat_list in next_techs.items():\n",
    "        next_idx = technique_to_idx[next_tech]\n",
    "        \n",
    "        # Aggregate network features for this transition\n",
    "        net_feat_array = np.array(net_feat_list)\n",
    "        \n",
    "        # Create 6D edge feature vector\n",
    "        edge_feature = [\n",
    "            len(net_feat_list) / total_transitions,  # Transition probability\n",
    "            net_feat_array[:, 0].mean(),             # Avg packet count\n",
    "            net_feat_array[:, 0].std() + 1e-8,       # Std packet count\n",
    "            net_feat_array[:, 1].mean(),             # Avg byte count\n",
    "            net_feat_array[:, 1].std() + 1e-8,       # Std byte count\n",
    "            np.log1p(len(net_feat_list))             # Log transition count\n",
    "        ]\n",
    "        \n",
    "        edge_list.append([curr_idx, next_idx])\n",
    "        edge_features.append(edge_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90e1040",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "edge_attr = torch.tensor(edge_features, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e6119a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize edge features\n",
    "edge_attr_mean = edge_attr.mean(dim=0, keepdim=True)\n",
    "edge_attr_std = edge_attr.std(dim=0, keepdim=True) + 1e-8\n",
    "edge_attr = (edge_attr - edge_attr_mean) / edge_attr_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8adffef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_feature_dim = edge_attr.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec18bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"âœ… Graph: {num_techniques} nodes, {edge_index.shape[1]} edges\")\n",
    "print(f\"âœ… Edge features: {edge_feature_dim}D (probability + network stats)\")\n",
    "print(f\"\\nEdge feature breakdown:\")\n",
    "print(f\"  [0] Transition probability\")\n",
    "print(f\"  [1] Avg packet count\")\n",
    "print(f\"  [2] Std packet count\")\n",
    "print(f\"  [3] Avg byte count\")\n",
    "print(f\"  [4] Std byte count\")\n",
    "print(f\"  [5] Log transition count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a31b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# PREPARE DATA\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"PREPARING DATA\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe38514",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_indices = torch.tensor([technique_to_idx[p['current_attack']] for p in training_pairs], dtype=torch.long)\n",
    "next_indices = torch.tensor([technique_to_idx[p['next_attack']] for p in training_pairs], dtype=torch.long)\n",
    "network_features_tensor = torch.tensor([p['network_features'] for p in training_pairs], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1ee909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "mean = network_features_tensor.mean(dim=0, keepdim=True)\n",
    "std = network_features_tensor.std(dim=0, keepdim=True) + 1e-8\n",
    "network_features_tensor = (network_features_tensor - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed9aefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "n = len(training_pairs)\n",
    "indices = torch.randperm(n)\n",
    "train_size, val_size = int(0.7 * n), int(0.15 * n)\n",
    "train_idx = indices[:train_size]\n",
    "val_idx = indices[train_size:train_size + val_size]\n",
    "test_idx = indices[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ef80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ… Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057ce920",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# NETWORK-AWARE MODEL\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DEFINING NETWORK-AWARE MODEL\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28459eff",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class NetworkAwareHybridGNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Enhanced Hybrid GNN with network-aware graph edges.\n",
    "    \n",
    "    Key improvement: GAT layers now use multi-dimensional edge features\n",
    "    that include network statistics (packet/byte counts) for each transition.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_nodes, num_network_features, edge_feature_dim=6, \n",
    "                 hidden_dim=256, num_heads=8, dropout=0.4):\n",
    "        super().__init__()\n",
    "        self.node_embedding = nn.Embedding(num_nodes, hidden_dim)\n",
    "        \n",
    "        # 3-layer network encoder with batch norm (for current observation)\n",
    "        self.network_encoder = nn.Sequential(\n",
    "            nn.Linear(num_network_features, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # 3 GAT layers with edge features (NETWORK-AWARE!)\n",
    "        self.gat1 = GATConv(\n",
    "            hidden_dim, \n",
    "            hidden_dim // num_heads, \n",
    "            heads=num_heads, \n",
    "            edge_dim=edge_feature_dim,  # Now uses edge features!\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.gat2 = GATConv(\n",
    "            hidden_dim, \n",
    "            hidden_dim // num_heads, \n",
    "            heads=num_heads, \n",
    "            edge_dim=edge_feature_dim,  # Network-aware attention\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.gat3 = GATConv(\n",
    "            hidden_dim, \n",
    "            hidden_dim, \n",
    "            heads=1, \n",
    "            edge_dim=edge_feature_dim,  # Final layer also uses edge features\n",
    "            dropout=dropout\n",
    "        )\n",
    "        \n",
    "        # Fusion with batch norm\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * 2, hidden_dim),\n",
    "            nn.BatchNorm1d(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Linear(hidden_dim, num_nodes)\n",
    "        \n",
    "    def forward(self, current_node_ids, network_features, edge_index, edge_attr):\n",
    "        # Graph convolution with network-aware edges\n",
    "        x = self.node_embedding.weight\n",
    "        \n",
    "        # GAT layers now use edge_attr (network statistics)\n",
    "        x = F.relu(self.gat1(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = F.relu(self.gat2(x, edge_index, edge_attr))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.gat3(x, edge_index, edge_attr)\n",
    "        \n",
    "        # Extract embeddings for current nodes\n",
    "        node_emb = x[current_node_ids]\n",
    "        \n",
    "        # Encode current network observation\n",
    "        net_emb = self.network_encoder(network_features)\n",
    "        \n",
    "        # Fuse graph and network information\n",
    "        combined = torch.cat([node_emb, net_emb], dim=1)\n",
    "        fused = self.fusion(combined)\n",
    "        logits = self.classifier(fused)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdd470e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NetworkAwareHybridGNN(\n",
    "    num_techniques, \n",
    "    num_network_features, \n",
    "    edge_feature_dim,\n",
    "    HIDDEN_DIM, \n",
    "    NUM_HEADS, \n",
    "    DROPOUT\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690b97ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nâœ… Model: {num_params:,} parameters\")\n",
    "print(f\"âœ… Edge feature dimension: {edge_feature_dim}D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5785f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TRAINING\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TRAINING\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54904c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6191fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index = edge_index.to(device)\n",
    "edge_attr = edge_attr.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8822a5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "Path('models').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01cf03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track training history\n",
    "train_losses, train_accs = [], []\n",
    "val_losses, val_accs = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56362286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸš€ Training started...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "956802d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    perm = torch.randperm(len(train_idx))\n",
    "    shuffled_train_idx = train_idx[perm]\n",
    "    \n",
    "    epoch_loss, epoch_correct, num_batches = 0, 0, 0\n",
    "    \n",
    "    for i in range(0, len(shuffled_train_idx), BATCH_SIZE):\n",
    "        batch_idx = shuffled_train_idx[i:i + BATCH_SIZE]\n",
    "        curr = current_indices[batch_idx].to(device)\n",
    "        nxt = next_indices[batch_idx].to(device)\n",
    "        net_feat = network_features_tensor[batch_idx].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        logits = model(curr, net_feat, edge_index, edge_attr)\n",
    "        loss = criterion(logits, nxt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        epoch_correct += (predicted == nxt).sum().item()\n",
    "        num_batches += 1\n",
    "    \n",
    "    train_loss = epoch_loss / num_batches\n",
    "    train_acc = epoch_correct / len(train_idx)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        curr = current_indices[val_idx].to(device)\n",
    "        nxt = next_indices[val_idx].to(device)\n",
    "        net_feat = network_features_tensor[val_idx].to(device)\n",
    "        \n",
    "        logits = model(curr, net_feat, edge_index, edge_attr)\n",
    "        val_loss = criterion(logits, nxt)\n",
    "        \n",
    "        _, predicted = torch.max(logits, 1)\n",
    "        val_acc = (predicted == nxt).float().mean().item()\n",
    "        \n",
    "        _, top3 = torch.topk(logits, 3, dim=1)\n",
    "        val_top3 = sum((nxt[i] in top3[i]) for i in range(len(nxt))) / len(nxt)\n",
    "        \n",
    "        _, top5 = torch.topk(logits, 5, dim=1)\n",
    "        val_top5 = sum((nxt[i] in top5[i]) for i in range(len(nxt))) / len(nxt)\n",
    "    \n",
    "    val_losses.append(val_loss.item())\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    scheduler.step(val_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch+1:3d}/{NUM_EPOCHS} | Train: {train_loss:.4f}/{train_acc:.4f} | \"\n",
    "              f\"Val: {val_loss.item():.4f}/{val_acc:.4f} Top3:{val_top3:.4f} Top5:{val_top5:.4f}\")\n",
    "    \n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), 'models/best_network_aware_model.pt')\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    if patience_counter >= EARLY_STOP_PATIENCE:\n",
    "        print(f\"\\nâš ï¸  Early stopping at epoch {epoch+1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925ade67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ… Training complete! Best val: {best_val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227534a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# TEST EVALUATION WITH METRICS\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TEST EVALUATION\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181503c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('models/best_network_aware_model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb29026",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    curr = current_indices[test_idx].to(device)\n",
    "    nxt = next_indices[test_idx].to(device)\n",
    "    net_feat = network_features_tensor[test_idx].to(device)\n",
    "    \n",
    "    logits = model(curr, net_feat, edge_index, edge_attr)\n",
    "    test_loss = criterion(logits, nxt)\n",
    "    \n",
    "    _, predicted = torch.max(logits, 1)\n",
    "    test_acc = (predicted == nxt).float().mean().item()\n",
    "    \n",
    "    _, top3 = torch.topk(logits, 3, dim=1)\n",
    "    test_top3 = sum((nxt[i] in top3[i]) for i in range(len(nxt))) / len(nxt)\n",
    "    \n",
    "    _, top5 = torch.topk(logits, 5, dim=1)\n",
    "    test_top5 = sum((nxt[i] in top5[i]) for i in range(len(nxt))) / len(nxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f1dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nðŸ“Š Test Results:\")\n",
    "print(f\"   Loss:      {test_loss.item():.4f}\")\n",
    "print(f\"   Top-1 Acc: {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "print(f\"   Top-3 Acc: {test_top3:.4f} ({test_top3*100:.2f}%)\")\n",
    "print(f\"   Top-5 Acc: {test_top5:.4f} ({test_top5*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33282a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-class analysis\n",
    "print(f\"\\nðŸ“ˆ Per-Class Analysis (Top 10 classes):\")\n",
    "predicted_cpu = predicted.cpu().numpy()\n",
    "nxt_cpu = nxt.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_correct = defaultdict(int)\n",
    "class_total = defaultdict(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71df886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pred, true in zip(predicted_cpu, nxt_cpu):\n",
    "    class_total[true] += 1\n",
    "    if pred == true:\n",
    "        class_correct[true] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53964a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_classes = sorted(class_total.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "for class_idx, total in sorted_classes:\n",
    "    correct = class_correct[class_idx]\n",
    "    acc = correct / total if total > 0 else 0\n",
    "    tech_name = idx_to_technique[class_idx]\n",
    "    print(f\"   {tech_name:10s}: {correct:3d}/{total:3d} = {acc:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdb279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save mappings and normalization parameters\n",
    "mappings = {\n",
    "    'technique_to_idx': technique_to_idx,\n",
    "    'idx_to_technique': idx_to_technique,\n",
    "    'num_techniques': num_techniques,\n",
    "    'num_network_features': num_network_features,\n",
    "    'edge_feature_dim': edge_feature_dim,\n",
    "    'network_mean': mean.numpy(),\n",
    "    'network_std': std.numpy(),\n",
    "    'edge_attr_mean': edge_attr_mean.numpy(),\n",
    "    'edge_attr_std': edge_attr_std.numpy()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfe1902",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/network_aware_mappings.pkl', 'wb') as f:\n",
    "    pickle.dump(mappings, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3958419f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save training history\n",
    "history = {\n",
    "    'train_losses': train_losses,\n",
    "    'train_accs': train_accs,\n",
    "    'val_losses': val_losses,\n",
    "    'val_accs': val_accs\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66ae2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('models/training_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b02120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nâœ… Model saved to: models/best_network_aware_model.pt\")\n",
    "print(f\"âœ… Mappings saved to: models/network_aware_mappings.pkl\")\n",
    "print(f\"âœ… Training history saved to: models/training_history.pkl\")\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc3ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# VISUALIZATION & ANALYSIS\n",
    "# ============================================================================\n",
    "if PLOT_AVAILABLE:\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"GENERATING FIGURES & ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create figures directory\n",
    "    Path('figures').mkdir(exist_ok=True)\n",
    "    \n",
    "    # ========== FIGURE 1: Training Curves ==========\n",
    "    print(\"\\nðŸ“Š Creating training curves...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Plot 1: Loss curves\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    axes[0, 0].plot(epochs, train_losses, 'b-', linewidth=2, label='Train Loss', marker='o', markersize=3)\n",
    "    axes[0, 0].plot(epochs, val_losses, 'r-', linewidth=2, label='Val Loss', marker='s', markersize=3)\n",
    "    axes[0, 0].set_title('Training & Validation Loss', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Accuracy curves\n",
    "    axes[0, 1].plot(epochs, train_accs, 'b-', linewidth=2, label='Train Acc', marker='o', markersize=3)\n",
    "    axes[0, 1].plot(epochs, val_accs, 'r-', linewidth=2, label='Val Acc', marker='s', markersize=3)\n",
    "    axes[0, 1].set_title('Training & Validation Accuracy', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Top-K Accuracy\n",
    "    top_k_accs = [test_acc, test_top3, test_top5]\n",
    "    top_k_labels = ['Top-1', 'Top-3', 'Top-5']\n",
    "    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "    \n",
    "    bars = axes[1, 0].bar(top_k_labels, top_k_accs, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "    axes[1, 0].set_title('Top-K Accuracy on Test Set', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_ylabel('Accuracy')\n",
    "    axes[1, 0].set_ylim([0, 1])\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    for bar, acc in zip(bars, top_k_accs):\n",
    "        height = bar.get_height()\n",
    "        axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,\n",
    "                       f'{acc:.3f}\\n({acc*100:.1f}%)',\n",
    "                       ha='center', va='bottom', fontweight='bold', fontsize=10)\n",
    "    \n",
    "    # Plot 4: Per-Class Accuracy (Top 10)\n",
    "    sorted_classes_list = sorted(class_total.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    class_names = [idx_to_technique[idx][:10] for idx, _ in sorted_classes_list]\n",
    "    class_accs = [class_correct[idx] / class_total[idx] if class_total[idx] > 0 else 0 \n",
    "                  for idx, _ in sorted_classes_list]\n",
    "    \n",
    "    bars = axes[1, 1].barh(class_names, class_accs, color='#95E1D3', alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "    axes[1, 1].set_title('Per-Class Accuracy (Top 10 Classes)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Accuracy')\n",
    "    axes[1, 1].set_xlim([0, 1])\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "    \n",
    "    for bar, acc in zip(bars, class_accs):\n",
    "        width = bar.get_width()\n",
    "        axes[1, 1].text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                       f' {acc:.3f}',\n",
    "                       ha='left', va='center', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/network_aware_training.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: figures/network_aware_training.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== FIGURE 2: Edge Feature Analysis ==========\n",
    "    print(\"\\nðŸ“Š Creating edge feature analysis...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    \n",
    "    edge_attr_np = edge_attr.cpu().numpy()\n",
    "    feature_names = ['Trans. Prob', 'Avg Packets', 'Std Packets', 'Avg Bytes', 'Std Bytes', 'Log Count']\n",
    "    \n",
    "    for idx, (ax, name) in enumerate(zip(axes.flat, feature_names)):\n",
    "        ax.hist(edge_attr_np[:, idx], bins=50, color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "        ax.set_title(f'{name} Distribution', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Normalized Value')\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add statistics\n",
    "        mean_val = edge_attr_np[:, idx].mean()\n",
    "        std_val = edge_attr_np[:, idx].std()\n",
    "        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "        ax.legend(fontsize=9)\n",
    "    \n",
    "    plt.suptitle('Network-Aware Edge Features Distribution', fontsize=16, fontweight='bold', y=1.00)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/edge_features_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: figures/edge_features_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== FIGURE 3: Network Feature Variance Analysis ==========\n",
    "    print(\"\\nðŸ“Š Creating network variance analysis...\")\n",
    "    \n",
    "    # Analyze which transitions have high network variance\n",
    "    high_variance_transitions = []\n",
    "    \n",
    "    for (curr_tech, next_tech), net_feat_list in edge_features_dict.items():\n",
    "        if len(net_feat_list) >= 5:  # Only consider transitions with enough samples\n",
    "            net_feat_array = np.array(net_feat_list)\n",
    "            \n",
    "            avg_packets = net_feat_array[:, 0].mean()\n",
    "            std_packets = net_feat_array[:, 0].std()\n",
    "            avg_bytes = net_feat_array[:, 1].mean()\n",
    "            std_bytes = net_feat_array[:, 1].std()\n",
    "            \n",
    "            cv_packets = std_packets / (avg_packets + 1e-8)  # Coefficient of variation\n",
    "            cv_bytes = std_bytes / (avg_bytes + 1e-8)\n",
    "            \n",
    "            high_variance_transitions.append({\n",
    "                'transition': f\"{curr_tech[:8]}â†’{next_tech[:8]}\",\n",
    "                'cv_packets': cv_packets,\n",
    "                'cv_bytes': cv_bytes,\n",
    "                'avg_packets': avg_packets,\n",
    "                'std_packets': std_packets,\n",
    "                'avg_bytes': avg_bytes,\n",
    "                'std_bytes': std_bytes,\n",
    "                'count': len(net_feat_list)\n",
    "            })\n",
    "    \n",
    "    high_variance_transitions.sort(key=lambda x: x['cv_packets'], reverse=True)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Top 15 transitions by packet variance\n",
    "    top_15 = high_variance_transitions[:15]\n",
    "    trans_names = [t['transition'] for t in top_15]\n",
    "    cv_packets = [t['cv_packets'] for t in top_15]\n",
    "    \n",
    "    bars = axes[0].barh(trans_names, cv_packets, color='#FF6B6B', alpha=0.8, edgecolor='black')\n",
    "    axes[0].set_title('Top 15 Transitions by Packet Count Variance', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Coefficient of Variation (CV)')\n",
    "    axes[0].grid(True, alpha=0.3, axis='x')\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    for bar, cv in zip(bars, cv_packets):\n",
    "        width = bar.get_width()\n",
    "        axes[0].text(width, bar.get_y() + bar.get_height()/2.,\n",
    "                    f' {cv:.2f}',\n",
    "                    ha='left', va='center', fontweight='bold', fontsize=8)\n",
    "    \n",
    "    # Plot 2: Scatter plot - Packet variance vs Byte variance\n",
    "    all_cv_packets = [t['cv_packets'] for t in high_variance_transitions]\n",
    "    all_cv_bytes = [t['cv_bytes'] for t in high_variance_transitions]\n",
    "    all_counts = [t['count'] for t in high_variance_transitions]\n",
    "    \n",
    "    scatter = axes[1].scatter(all_cv_packets, all_cv_bytes, \n",
    "                             s=[c*2 for c in all_counts], \n",
    "                             c=all_counts, cmap='viridis', \n",
    "                             alpha=0.6, edgecolors='black', linewidth=0.5)\n",
    "    axes[1].set_title('Network Variance Correlation', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Packet Count CV')\n",
    "    axes[1].set_ylabel('Byte Count CV')\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=axes[1])\n",
    "    cbar.set_label('Transition Count', rotation=270, labelpad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/network_variance_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: figures/network_variance_analysis.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== FIGURE 4: Confusion Matrix ==========\n",
    "    print(\"\\nðŸ“Š Creating confusion matrix...\")\n",
    "    \n",
    "    # Get top 15 most frequent classes\n",
    "    top_15_classes = [idx for idx, _ in sorted(class_total.items(), key=lambda x: x[1], reverse=True)[:15]]\n",
    "    \n",
    "    # Filter predictions\n",
    "    mask = np.isin(nxt_cpu, top_15_classes)\n",
    "    filtered_true = nxt_cpu[mask]\n",
    "    filtered_pred = predicted_cpu[mask]\n",
    "    \n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(filtered_true, filtered_pred, labels=top_15_classes)\n",
    "    cm_normalized = cm.astype('float') / (cm.sum(axis=1)[:, np.newaxis] + 1e-8)\n",
    "    cm_normalized = np.nan_to_num(cm_normalized)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    im = ax.imshow(cm_normalized, interpolation='nearest', cmap='YlOrRd')\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    class_labels = [idx_to_technique[idx][:10] for idx in top_15_classes]\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels=class_labels,\n",
    "           yticklabels=class_labels,\n",
    "           title='Confusion Matrix (Top 15 Classes, Normalized)',\n",
    "           ylabel='True Technique',\n",
    "           xlabel='Predicted Technique')\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Add text annotations\n",
    "    thresh = cm_normalized.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            if cm_normalized[i, j] > 0.01:\n",
    "                ax.text(j, i, f'{cm_normalized[i, j]:.2f}',\n",
    "                       ha=\"center\", va=\"center\",\n",
    "                       color=\"white\" if cm_normalized[i, j] > thresh else \"black\",\n",
    "                       fontsize=7)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/network_aware_confusion_matrix.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: figures/network_aware_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== FIGURE 5: Prediction Confidence Analysis ==========\n",
    "    print(\"\\nðŸ“Š Creating prediction confidence analysis...\")\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        curr = current_indices[test_idx].to(device)\n",
    "        nxt = next_indices[test_idx].to(device)\n",
    "        net_feat = network_features_tensor[test_idx].to(device)\n",
    "        \n",
    "        logits = model(curr, net_feat, edge_index, edge_attr)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        \n",
    "        max_probs, _ = torch.max(probs, dim=1)\n",
    "        max_probs_cpu = max_probs.cpu().numpy()\n",
    "        \n",
    "        correct_mask = (predicted == nxt).cpu().numpy()\n",
    "        correct_probs = max_probs_cpu[correct_mask]\n",
    "        incorrect_probs = max_probs_cpu[~correct_mask]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Plot 1: Confidence distribution\n",
    "    axes[0].hist(correct_probs, bins=30, alpha=0.7, label='Correct', color='#4ECDC4', edgecolor='black')\n",
    "    axes[0].hist(incorrect_probs, bins=30, alpha=0.7, label='Incorrect', color='#FF6B6B', edgecolor='black')\n",
    "    axes[0].set_title('Prediction Confidence Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Confidence (Max Probability)')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].legend(fontsize=11)\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add statistics\n",
    "    axes[0].axvline(correct_probs.mean(), color='#4ECDC4', linestyle='--', linewidth=2, \n",
    "                   label=f'Correct Mean: {correct_probs.mean():.3f}')\n",
    "    axes[0].axvline(incorrect_probs.mean(), color='#FF6B6B', linestyle='--', linewidth=2,\n",
    "                   label=f'Incorrect Mean: {incorrect_probs.mean():.3f}')\n",
    "    \n",
    "    # Plot 2: Calibration curve\n",
    "    confidence_bins = np.linspace(0, 1, 11)\n",
    "    bin_centers = (confidence_bins[:-1] + confidence_bins[1:]) / 2\n",
    "    bin_accuracies = []\n",
    "    bin_counts = []\n",
    "    \n",
    "    for i in range(len(confidence_bins) - 1):\n",
    "        mask = (max_probs_cpu >= confidence_bins[i]) & (max_probs_cpu < confidence_bins[i+1])\n",
    "        if mask.sum() > 0:\n",
    "            acc = correct_mask[mask].mean()\n",
    "            bin_accuracies.append(acc)\n",
    "            bin_counts.append(mask.sum())\n",
    "        else:\n",
    "            bin_accuracies.append(0)\n",
    "            bin_counts.append(0)\n",
    "    \n",
    "    axes[1].plot(bin_centers, bin_accuracies, marker='o', linewidth=2, markersize=10, \n",
    "                color='#45B7D1', label='Model Calibration')\n",
    "    axes[1].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Perfect Calibration')\n",
    "    axes[1].set_title('Model Calibration Curve', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Confidence')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend(fontsize=11)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].set_xlim([0, 1])\n",
    "    axes[1].set_ylim([0, 1])\n",
    "    \n",
    "    # Add sample counts\n",
    "    for x, y, count in zip(bin_centers, bin_accuracies, bin_counts):\n",
    "        if count > 0:\n",
    "            axes[1].text(x, y + 0.03, f'n={count}', ha='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/network_aware_confidence.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: figures/network_aware_confidence.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== FIGURE 6: Data Distribution Analysis ==========\n",
    "    print(\"\\nðŸ“Š Creating data distribution analysis...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    \n",
    "    # Plot 1: Variant Distribution\n",
    "    variants = list(variant_counts.keys())\n",
    "    counts = list(variant_counts.values())\n",
    "    colors_var = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#95E1D3', '#FFE66D'][:len(variants)]\n",
    "    \n",
    "    wedges, texts, autotexts = axes[0, 0].pie(counts, labels=variants, autopct='%1.1f%%',\n",
    "                                               colors=colors_var, startangle=90)\n",
    "    axes[0, 0].set_title('Variant Type Distribution', fontsize=14, fontweight='bold')\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('white')\n",
    "        autotext.set_fontweight('bold')\n",
    "        autotext.set_fontsize(10)\n",
    "    \n",
    "    # Plot 2: Transition Frequency Distribution\n",
    "    transition_freqs = [count for _, count in transition_counts.most_common()]\n",
    "    axes[0, 1].hist(transition_freqs, bins=50, color='#4ECDC4', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 1].set_title('Transition Frequency Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0, 1].set_xlabel('Frequency')\n",
    "    axes[0, 1].set_ylabel('Number of Transitions')\n",
    "    axes[0, 1].set_yscale('log')\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 3: Network Feature Distribution (Packet Count)\n",
    "    axes[1, 0].hist(all_features[:, 0], bins=50, color='#FF6B6B', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 0].set_title('Packet Count Distribution (3s window)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Packet Count')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1, 0].axvline(all_features[:, 0].mean(), color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Mean: {all_features[:, 0].mean():.1f}')\n",
    "    axes[1, 0].legend()\n",
    "    \n",
    "    # Plot 4: Network Feature Distribution (Byte Count)\n",
    "    axes[1, 1].hist(all_features[:, 1], bins=50, color='#95E1D3', alpha=0.7, edgecolor='black')\n",
    "    axes[1, 1].set_title('Byte Count Distribution (3s window)', fontsize=14, fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Byte Count')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1, 1].axvline(all_features[:, 1].mean(), color='red', linestyle='--', \n",
    "                      linewidth=2, label=f'Mean: {all_features[:, 1].mean():.1f}')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/data_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: figures/data_distribution.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== FIGURE 7: Graph Structure Visualization ==========\n",
    "    print(\"\\nðŸ“Š Creating graph structure visualization...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Degree distribution\n",
    "    in_degrees = defaultdict(int)\n",
    "    out_degrees = defaultdict(int)\n",
    "    \n",
    "    for i in range(edge_index.shape[1]):\n",
    "        src = edge_index[0, i].item()\n",
    "        dst = edge_index[1, i].item()\n",
    "        out_degrees[src] += 1\n",
    "        in_degrees[dst] += 1\n",
    "    \n",
    "    in_deg_values = list(in_degrees.values())\n",
    "    out_deg_values = list(out_degrees.values())\n",
    "    \n",
    "    axes[0].hist(in_deg_values, bins=30, alpha=0.7, label='In-degree', color='#4ECDC4', edgecolor='black')\n",
    "    axes[0].hist(out_deg_values, bins=30, alpha=0.7, label='Out-degree', color='#FF6B6B', edgecolor='black')\n",
    "    axes[0].set_title('Node Degree Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xlabel('Degree')\n",
    "    axes[0].set_ylabel('Number of Nodes')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Edge weight distribution (transition probability)\n",
    "    edge_probs = edge_attr[:, 0].cpu().numpy()  # First feature is transition probability\n",
    "    axes[1].hist(edge_probs, bins=50, color='#95E1D3', alpha=0.7, edgecolor='black')\n",
    "    axes[1].set_title('Edge Weight Distribution (Normalized)', fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xlabel('Normalized Transition Probability')\n",
    "    axes[1].set_ylabel('Number of Edges')\n",
    "    axes[1].grid(True, alpha=0.3, axis='y')\n",
    "    axes[1].axvline(edge_probs.mean(), color='red', linestyle='--', \n",
    "                   linewidth=2, label=f'Mean: {edge_probs.mean():.2f}')\n",
    "    axes[1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('figures/graph_structure.png', dpi=300, bbox_inches='tight')\n",
    "    print(\"âœ… Saved: figures/graph_structure.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # ========== Save High Variance Transitions Report ==========\n",
    "    print(\"\\nðŸ“Š Saving high variance transitions report...\")\n",
    "    \n",
    "    with open('figures/high_variance_transitions.txt', 'w') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"HIGH VARIANCE TRANSITIONS REPORT\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        f.write(\"These transitions show high variability in network features,\\n\")\n",
    "        f.write(\"indicating they benefit most from network-aware graph edges.\\n\\n\")\n",
    "        \n",
    "        f.write(f\"{'Rank':<6}{'Transition':<30}{'CV Packets':<12}{'CV Bytes':<12}{'Count':<8}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n",
    "        \n",
    "        for i, trans in enumerate(high_variance_transitions[:30], 1):\n",
    "            f.write(f\"{i:<6}{trans['transition']:<30}{trans['cv_packets']:<12.3f}\"\n",
    "                   f\"{trans['cv_bytes']:<12.3f}{trans['count']:<8}\\n\")\n",
    "        \n",
    "        f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "        f.write(\"STATISTICS\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        all_cv_packets = [t['cv_packets'] for t in high_variance_transitions]\n",
    "        all_cv_bytes = [t['cv_bytes'] for t in high_variance_transitions]\n",
    "        \n",
    "        f.write(f\"Total transitions analyzed: {len(high_variance_transitions)}\\n\")\n",
    "        f.write(f\"Packet CV - Mean: {np.mean(all_cv_packets):.3f}, Median: {np.median(all_cv_packets):.3f}\\n\")\n",
    "        f.write(f\"Byte CV - Mean: {np.mean(all_cv_bytes):.3f}, Median: {np.median(all_cv_bytes):.3f}\\n\")\n",
    "        f.write(f\"\\nTransitions with CV > 0.5 (high variance): {sum(1 for cv in all_cv_packets if cv > 0.5)}\\n\")\n",
    "        f.write(f\"Transitions with CV > 1.0 (very high variance): {sum(1 for cv in all_cv_packets if cv > 1.0)}\\n\")\n",
    "    \n",
    "    print(\"âœ… Saved: figures/high_variance_transitions.txt\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"FIGURES SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nâœ… Generated 7 evaluation figures:\")\n",
    "    print(\"   1. figures/network_aware_training.png - Training curves & accuracies\")\n",
    "    print(\"   2. figures/edge_features_distribution.png - Edge feature analysis\")\n",
    "    print(\"   3. figures/network_variance_analysis.png - Network variance patterns\")\n",
    "    print(\"   4. figures/network_aware_confusion_matrix.png - Confusion matrix\")\n",
    "    print(\"   5. figures/network_aware_confidence.png - Confidence analysis\")\n",
    "    print(\"   6. figures/data_distribution.png - Data distribution plots\")\n",
    "    print(\"   7. figures/graph_structure.png - Graph topology analysis\")\n",
    "    print(\"\\nâœ… Generated 1 analysis report:\")\n",
    "    print(\"   - figures/high_variance_transitions.txt - High variance transitions\")\n",
    "    print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f25e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "else:\n",
    "    print(\"\\nâš ï¸  Visualization skipped (matplotlib not available)\")\n",
    "    print(\"   Install with: pip install matplotlib seaborn scikit-learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a2ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nðŸŽ‰ Network-aware training complete!\")\n",
    "print(\"\\nðŸ’¡ Key Improvements:\")\n",
    "print(\"   - Graph edges now contain 6D features (not just 1D frequency)\")\n",
    "print(\"   - GAT layers use network statistics for attention computation\")\n",
    "print(\"   - Model can distinguish high-traffic vs low-traffic transitions\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
